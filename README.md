# äººå·¥æ™ºèƒ½ä¹‹æœºå™¨å­¦ä¹ -XGBoostå­¦ä¹ ä¸ä½¿ç”¨

category: ai
type: Post
status: Published
date: 2025/07/14
slug: /ai/base/ml/XGBoost
summary: XGBoostå­¦ä¹ ä¸ä½¿ç”¨
tags: å†³ç­–æ ‘, æœºå™¨å­¦ä¹ , é›†æˆå­¦ä¹ 

<aside>
ğŸ’¡ *æˆ‘æ˜¯ä»»ä¼ŸRaveyï¼Œ*10å¹´èµ„æ·±ç¼–ç¨‹å¼€å‘ç»éªŒçš„å·¥ç¨‹å¸ˆï¼Œå¸¦ä½ èµ°è¿›aiçš„ä¸–ç•Œã€‚å¦‚æœä½ è§‰å¾—æ–‡ç« å¯¹ä½ æœ‰å¸®åŠ©*ï¼Œä¹Ÿæ¬¢è¿ä½ å’Œæˆ‘ä¸€èµ·æ¢è®¨aiæŠ€æœ¯ã€‚æˆ‘çš„é‚®ç®±rw_3306@163.comï¼Œå¾®ä¿¡å·Ravey6379ï¼Œåšå®¢ï¼šhttps://blog.ravey.site*

</aside>

## èƒŒæ™¯

æœ€è¿‘åœ¨åšä¸€ä¸ªé¡¹ç›®ï¼Œåœ¨å½“å‰çš„å•†ä¸šç¯å¢ƒä¸­ï¼Œä¼ä¸šé¢ä¸´ç€å¤§é‡çš„å¤–åŒ…é¡¹ç›®å¼€å‘éœ€æ±‚ã€‚å½“æ¶‰åŠç¬¬ä¸‰æ–¹ç³»ç»Ÿäº¤ä»˜æ—¶ï¼Œç¬¬ä¸‰æ–¹ä¾›åº”å•†ä¼šæä¾›å·¥æ—¶è¯„ä¼°å’ŒæŠ¥ä»·ã€‚ç„¶è€Œï¼Œå¦‚ä½•åˆ¤æ–­è¿™äº›å·¥æ—¶å’Œä»·æ ¼çš„åˆç†æ€§ä¸€ç›´æ˜¯ä¼ä¸šç®¡ç†ä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¾èµ–äºç»éªŒåˆ¤æ–­ï¼Œç¼ºä¹å®¢è§‚çš„æ•°æ®æ”¯æ’‘ï¼Œå®¹æ˜“å¯¼è‡´æˆæœ¬æ§åˆ¶ä¸å½“å’Œå†³ç­–å¤±è¯¯ã€‚é¡¹ç›®çš„ç›®æ ‡æ˜¯ï¼Œ**æ„å»ºä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„æ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ**ï¼Œé€šè¿‡åˆ†æä¼ä¸šç§¯ç´¯çš„å¤§é‡å†å²æŠ¥ä»·å’Œæˆäº¤åˆåŒæ•°æ®ï¼Œä¸ºæ–°çš„é¡¹ç›®æŠ¥ä»·æä¾›å®¢è§‚ã€å‡†ç¡®çš„åˆç†æ€§åˆ¤æ–­ã€‚è¯¥ç³»ç»Ÿä¸ä»…é€‚ç”¨äºITå¼€å‘é¡¹ç›®ï¼Œè¿˜å¯ä»¥æ‰©å±•åˆ°ä¼ä¸šå…¶ä»–ä¸šåŠ¡é¢†åŸŸçš„æŠ¥ä»·è¯„ä¼°ã€‚

æˆ‘ä»¬çŸ¥é“æ•°æ®å°†åŒ…å«æ•°å€¼å‹ã€ç±»åˆ«å‹å’Œæ–‡æœ¬å‹ç‰¹å¾ï¼Œå¹¶ä¸”ç›®æ ‡æ˜¯é¢„æµ‹è¿ç»­å€¼ï¼ˆå·¥æ—¶å’Œä»·æ ¼ï¼‰ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ª**å›å½’é—®é¢˜**ã€‚è€ƒè™‘åˆ°ä¸šåŠ¡åœºæ™¯å¯¹æ¨¡å‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„è¦æ±‚ï¼Œæˆ‘ä»¬å°†é‡ç‚¹è€ƒè™‘ä»¥ä¸‹å‡ ç±»æ¨¡å‹ï¼š

1. **é›†æˆå­¦ä¹ æ¨¡å‹ (Ensemble Learning Models)ï¼š**
    - **ä¼˜åŠ¿ï¼š**Â åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé²æ£’æ€§å¼ºï¼Œå¯¹å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼æœ‰ä¸€å®šçš„å®¹å¿åº¦ï¼Œä¸”é€šå¸¸å…·æœ‰è¾ƒå¥½çš„å¯è§£é‡Šæ€§ï¼ˆå¯ä»¥é€šè¿‡ç‰¹å¾é‡è¦æ€§æ¥ç†è§£æ¨¡å‹å†³ç­–ï¼‰ã€‚
    - **å€™é€‰æ¨¡å‹ï¼š**
        - **XGBoost (eXtreme Gradient Boosting)ï¼š**Â æ€§èƒ½å¼ºå¤§ï¼Œå¹¿æ³›åº”ç”¨äºå„ç§å›å½’å’Œåˆ†ç±»ä»»åŠ¡ï¼Œæ”¯æŒå¹¶è¡Œè®¡ç®—ï¼Œå…·æœ‰æ­£åˆ™åŒ–åŠŸèƒ½ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
        - **LightGBM (Light Gradient Boosting Machine)ï¼š**Â å¾®è½¯å¼€å‘ï¼Œåœ¨é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢ä¼˜äºXGBoostï¼Œå°¤å…¶é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ã€‚
        - **Random Forest (éšæœºæ£®æ—)ï¼š**Â é€šè¿‡æ„å»ºå¤šæ£µå†³ç­–æ ‘å¹¶å–å¹³å‡æ¥æé«˜é¢„æµ‹ç²¾åº¦å’Œç¨³å®šæ€§ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆï¼Œå¯è§£é‡Šæ€§å¥½ã€‚
2. **çº¿æ€§æ¨¡å‹ (Linear Models)ï¼š**
    - **ä¼˜åŠ¿ï¼š**Â ç®€å•ã€å¿«é€Ÿã€å¯è§£é‡Šæ€§æå¼ºï¼Œå¯ä»¥ä½œä¸ºåŸºçº¿æ¨¡å‹ã€‚
    - **å€™é€‰æ¨¡å‹ï¼š**
        - **Linear Regression (çº¿æ€§å›å½’)ï¼š**Â æœ€åŸºæœ¬çš„å›å½’æ¨¡å‹ï¼Œé€‚ç”¨äºç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»çš„æƒ…å†µã€‚
        - **Ridge/Lasso Regression (å²­å›å½’/Lassoå›å½’)ï¼š**Â å¸¦æœ‰æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒLassoè¿˜èƒ½è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚
3. **ç¥ç»ç½‘ç»œ (Neural Networks)ï¼š**
    - **ä¼˜åŠ¿ï¼š**Â èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œå°¤å…¶åœ¨å¤„ç†å¤§é‡æ•°æ®å’Œé«˜ç»´ç‰¹å¾æ—¶è¡¨ç°ä¼˜å¼‚ã€‚å¯¹äºæ–‡æœ¬ç‰¹å¾ï¼ˆé€šè¿‡åµŒå…¥å±‚ï¼‰æœ‰å¤©ç„¶çš„ä¼˜åŠ¿ã€‚
    - **å€™é€‰æ¨¡å‹ï¼š**
        - **å¤šå±‚æ„ŸçŸ¥æœº (MLP)ï¼š**Â é€‚ç”¨äºç»“æ„åŒ–æ•°æ®ï¼Œå¯ä»¥é€šè¿‡å¢åŠ å±‚æ•°å’Œç¥ç»å…ƒæ•°é‡æ¥æ•æ‰å¤æ‚æ¨¡å¼ã€‚
        - **ç»“åˆæ–‡æœ¬åµŒå…¥çš„ç¥ç»ç½‘ç»œï¼š**Â å¦‚æœæ–‡æœ¬ç‰¹å¾ï¼ˆé¡¹ç›®æè¿°ï¼‰å¯¹é¢„æµ‹ç»“æœå½±å“æ˜¾è‘—ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆå¦‚BERTã€Word2Vecï¼‰å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œç„¶åå°†è¿™äº›å‘é‡ä¸å…¶ä»–ç»“æ„åŒ–ç‰¹å¾ä¸€èµ·è¾“å…¥åˆ°MLPä¸­ã€‚

**åˆæ­¥é€‰æ‹©ï¼š**Â è€ƒè™‘åˆ°é¡¹ç›®çš„å®é™…éœ€æ±‚ï¼Œæˆ‘ä»¬å€¾å‘äºä¼˜å…ˆå°è¯•**é›†æˆå­¦ä¹ æ¨¡å‹ï¼ˆXGBoostï¼‰**ã€‚å®ƒä»¬åœ¨å®é™…é¡¹ç›®ä¸­è¡¨ç°ç¨³å®šï¼Œæ€§èƒ½ä¼˜å¼‚ï¼Œå¹¶ä¸”æä¾›äº†ç‰¹å¾é‡è¦æ€§ç­‰å¯è§£é‡Šæ€§å·¥å…·ï¼Œè¿™å¯¹äºä¸šåŠ¡äººå‘˜ç†è§£æ¨¡å‹å†³ç­–å’Œè¿›è¡ŒæŠ¥ä»·åˆ†æè‡³å…³é‡è¦ã€‚åŒæ—¶ï¼Œå¯ä»¥è€ƒè™‘å°†**çº¿æ€§å›å½’**ä½œä¸ºåŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚å¦‚æœæ–‡æœ¬ç‰¹å¾çš„é‡è¦æ€§éå¸¸é«˜ï¼Œä¸”é›†æˆæ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œå†è€ƒè™‘å¼•å…¥**ç¥ç»ç½‘ç»œ**

## é›†æˆå­¦ä¹ æ¨¡å‹

åœ¨åŒ»å­¦è¯Šæ–­ä¸­ï¼Œå¦‚æœä¸€ä¸ªç—…äººçš„ç—…æƒ…å¤æ‚ï¼Œé€šå¸¸ä¸ä¼šåªè®©ä¸€ä¸ªåŒ»ç”Ÿæ‹æ¿ï¼Œè€Œæ˜¯ç»„ç»‡ **å¤šä½ä¸“å®¶ç»„æˆçš„ä¼šè¯Šå°ç»„**ï¼Œä»–ä»¬æ¥è‡ªä¸åŒé¢†åŸŸï¼ˆå¦‚æ”¾å°„ç§‘ã€å¤–ç§‘ã€è‚¿ç˜¤ç§‘ã€å…ç–«å­¦ç­‰ï¼‰ï¼Œæ¯äººä»è‡ªå·±çš„è§’åº¦ç»™å‡ºåˆ¤æ–­ï¼Œæœ€å **ç»¼åˆæ‰€æœ‰ä¸“å®¶æ„è§**ï¼Œåšå‡ºæœ€ç¨³å¦¥ã€å‡†ç¡®çš„è¯Šæ–­å†³ç­–ã€‚

å†ä¸¾ä¸€ä¸ªæ¯”è¾ƒè´´åˆ‡çš„ä¾‹å­ï¼š

> åœ¨é£é«˜æµªæ€¥çš„æµ·é¢ï¼Œä¸€è‰˜èˆ¹å®¹æ˜“åèˆªæˆ–å—æŸã€‚ä½†ä¸€æ”¯èˆ¹é˜Ÿï¼ˆå¦‚3~5è‰˜èˆ¹ï¼‰ç»“ä¼´èˆªè¡Œï¼Œå½¼æ­¤ä¿æŒé€šä¿¡å’Œæ„ŸçŸ¥å…±äº«ï¼š
> 
- æŸè‰˜èˆ¹åç¦»èˆªå‘ â†’ å…¶ä»–èˆ¹æ‹‰å›æ¥
- æœ‰èˆ¹è®¾å¤‡å¤±çµ â†’ å…¶ä»–èˆ¹å†—ä½™è¡¥ä½
- å¤šèˆ¹åŒæ—¶æ„ŸçŸ¥ã€åˆ¤æ–­é£æµªæ–¹å‘ â†’ æ›´ç¨³å¥å†³ç­–

æœ€ç»ˆç»“æœæ˜¯ï¼š**èˆ¹é˜Ÿæ¯”å•èˆ¹æ›´èƒ½ç©¿è¶Šæ¶åŠ£ç¯å¢ƒï¼ŒæˆåŠŸæ¦‚ç‡å¤§å¤§æå‡**

**é›†æˆå­¦ä¹ æ­£æ˜¯ä½¿ç”¨å¤šä¸ªä¸ªä½“å­¦ä¹ å™¨æ¥è·å¾—æ¯”æ¯ä¸ªå•ç‹¬å­¦ä¹ å™¨æ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚**

é›†æˆå­¦ä¹ æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šä¸ªä½“å­¦ä¹ å™¨é—´å­˜åœ¨å¼ºä¾èµ–å…³ç³»çš„**ä¸²è¡ŒåŒ–æ–¹æ³•ï¼Œæ•°æ®ä½¿ç”¨æœºåˆ¶è¢«ç§°ä¸ºæå‡ï¼ˆBoostingï¼‰å’Œä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´ä¸å­˜åœ¨å¼ºä¾èµ–å…³ç³»å¹¶è¡ŒåŒ–æ–¹æ³•ï¼Œæ•°æ®ä½¿ç”¨æœºåˆ¶è¢«ç§°ä¸ºæ‰“åŒ…ï¼ˆBaggingï¼‰**

## **XGBoost**

æˆ‘ä»¬ä»Šå¤©çš„ä¸»è§’**XGBoost**å°±å±äºé›†æˆå­¦ä¹ ä¸­çš„å¹¶è¡ŒåŒ–æ–¹æ³•**ã€‚**

**XGBoostï¼ˆExtreme Gradient Boostingï¼‰** æ˜¯ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•çš„ **æ¢¯åº¦æå‡æ ‘ï¼ˆGBDTï¼‰æ¡†æ¶**ï¼Œä»¥å…¶åœ¨ kaggle ç«èµ›å’Œå·¥ä¸šç•Œä¸­çš„å¼ºå¤§æ€§èƒ½è‘—ç§°ã€‚

- å±äº **é›†æˆå­¦ä¹ ä¸­çš„ Boosting æ–¹æ³•**
- æ ¸å¿ƒæ˜¯å¯¹ä¸€ç³»åˆ—å†³ç­–æ ‘çš„åŠ æƒç»„åˆ
- ä¼˜ç‚¹ï¼š
    - ç²¾åº¦é«˜ã€æ³›åŒ–èƒ½åŠ›å¼º
    - è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆæ”¯æŒå¹¶è¡Œã€ç¼“å­˜ä¼˜åŒ–ï¼‰
    - æ”¯æŒå¤„ç†ç¼ºå¤±å€¼ã€ç±»åˆ«ç‰¹å¾ã€ç‰¹å¾é‡è¦æ€§è¯„ä¼°
    - å¯ç”¨äºå›å½’ã€åˆ†ç±»ã€æ’åºç­‰ä»»åŠ¡

ä¸€èˆ¬ä¸­ç­‰è§„æ¨¡çš„æ•°æ®`10,000 è¡Œ Ã— 50 ç‰¹å¾`ï¼šé€‚åˆ XGBoostã€‚

### **XGBoostä¸­å¤šä¸ªä¸ªä½“å­¦ä¹ å™¨æ˜¯ä»€ä¹ˆï¼Ÿ**

åœ¨ **XGBoost** ä¸­ï¼Œå¤šä¸ªä¸ªä½“å­¦ä¹ å™¨ï¼ˆWeak Learnersï¼‰æŒ‡çš„æ˜¯ä¸€æ£µæ£µ**å›å½’æ ‘ï¼ˆCARTï¼šClassification And *Regression* Treesï¼‰**ã€‚è¿™äº›æ ‘é€šè¿‡ä¸²è¡Œâ€œçº é”™â€é€æ­¥å­¦ä¹ ï¼Œæœ€ç»ˆç»„æˆä¸€ä¸ªå¼ºé¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬æ¥è¯¦ç»†è®²æ¸…æ¥šå®ƒçš„ç»„æˆä¸å‚æ•°è°ƒæ§æ–¹å¼ï¼š

æœ¬è´¨ï¼š**ä¸€ç»„ä¸²è”æ„å»ºçš„å›å½’æ ‘ï¼ˆCARTï¼‰**

- æ¯ä¸€æ£µæ ‘éƒ½æ˜¯ä¸º**çº æ­£å‰é¢æ¨¡å‹çš„æ®‹å·®ï¼ˆé¢„æµ‹è¯¯å·®**ï¼‰è€Œè®­ç»ƒçš„
- æ¯æ£µæ ‘è¾“å‡ºä¸€ä¸ªé¢„æµ‹å€¼ï¼ˆä¸æ˜¯åˆ†ç±»ï¼Œè€Œæ˜¯æ•°å€¼ï¼‰
- æ‰€æœ‰æ ‘çš„åŠ æƒè¾“å‡ºä¹‹å’Œï¼Œæ‰æ˜¯æœ€ç»ˆé¢„æµ‹ç»“æœ

> è¿™å°±æ˜¯ Boosting çš„æ€æƒ³ï¼šåè€…æ”¹å‰è€…é”™ï¼Œåƒâ€œå¤è¯»æœºâ€ä¸€æ ·ä¸æ–­çº åã€‚
> 

---

### XGBoost æ ‘çš„è®­ç»ƒæµç¨‹ç®€åŒ–å›¾ç¤º

```python

  yçœŸå®å€¼ â‰ˆ Tree1(x) + Tree2(x) + Tree3(x) + ... + TreeN(x)
                â†‘         â†‘         â†‘           â†‘
            åˆæ­¥é¢„æµ‹   ä¿®æ­£è¯¯å·®   å†ä¿®æ­£       æœ€ç»ˆé¢„æµ‹
```

- ç¬¬1æ£µæ ‘å…ˆé¢„æµ‹
- åé¢çš„æ ‘çœ‹å‰é¢å“ªé‡Œé”™äº† â†’ é¢„æµ‹æ®‹å·® â†’ ä¸æ–­ä¿®æ­£
- æœ€ç»ˆè¾“å‡º = æ‰€æœ‰æ ‘çš„è¾“å‡ºä¹‹å’Œ

### å…³é”®å‚æ•°è§£é‡Šä¸è°ƒä¼˜å»ºè®®

**æ¨¡å‹ç»“æ„å‚æ•°ï¼ˆæ§åˆ¶ä¸ªä½“å­¦ä¹ å™¨ï¼‰**

| å‚æ•°å | è¯´æ˜ | å¸¸ç”¨å»ºè®®å€¼ |
| --- | --- | --- |
| `n_estimators` | æ ‘çš„æ•°é‡ï¼ˆä¹Ÿå« boosting roundï¼‰ | 100~1000ï¼Œéœ€è°ƒå‚ |
| `max_depth` | æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆå¤æ‚åº¦ï¼‰ | 3~10ï¼›è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆ |
| `learning_rate` | å­¦ä¹ ç‡ï¼ˆæ¯æ£µæ ‘çš„â€œæƒé‡â€ï¼‰ | é€šå¸¸ 0.01~0.3ï¼›è¶Šå°è¶Šç¨³ |
| `min_child_weight` | èŠ‚ç‚¹æœ€å°æ ·æœ¬æƒé‡å’Œï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ | é»˜è®¤1ï¼Œè¶Šå¤§è¶Šä¿å®ˆ |
| `gamma` | èŠ‚ç‚¹åˆ†è£‚æœ€å°æŸå¤±å‡å°‘å€¼ï¼ˆè¶Šå¤§è¶Šä¿å®ˆï¼‰ | 0~10ï¼Œå°æ•°æ®å¯ç•¥å¤§äº› |
| `subsample` | æ¯è½®è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬æ¯”ä¾‹ | 0.5~1.0ï¼›é˜²è¿‡æ‹Ÿåˆ |
| `colsample_bytree` | æ¯æ£µæ ‘ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹ | 0.5~1.0ï¼›é˜²è¿‡æ‹Ÿåˆ |
| `reg_alpha` / `reg_lambda` | L1 / L2 æ­£åˆ™ | é€šå¸¸è°ƒå¤§å¯é˜²è¿‡æ‹Ÿåˆ |

---

### ä¼˜åŒ–å‚æ•°ï¼ˆä¸è®­ç»ƒæ€§èƒ½ç›¸å…³ï¼‰

| å‚æ•°å | è¯´æ˜ | å¸¸ç”¨å€¼å»ºè®® |
| --- | --- | --- |
| `objective` | ç›®æ ‡å‡½æ•°ï¼Œå¦‚`reg:squarederror` | å›å½’é—®é¢˜å°±é€‰ squarederror |
| `n_jobs` | å¹¶è¡Œçº¿ç¨‹æ•° | -1 è¡¨ç¤ºè‡ªåŠ¨ä½¿ç”¨å…¨éƒ¨æ ¸å¿ƒ |
| `random_state` | æ§åˆ¶éšæœºæ€§ï¼Œæé«˜å¯å¤ç°æ€§ | å»ºè®®è®¾ä¸ºå›ºå®šå€¼ï¼ˆå¦‚ 42ï¼‰ |
|  |  |  |

---

### è°ƒå‚å»ºè®®ï¼ˆç»éªŒè·¯å¾„ï¼‰

| è°ƒå‚é¡ºåº | è°ƒæ•´ç›®æ ‡ | æ¨èåšæ³• |
| --- | --- | --- |
| ç¬¬ä¸€æ­¥ | æ‰¾åˆ°åˆé€‚çš„ `max_depth`, `min_child_weight` | æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ |
| ç¬¬äºŒæ­¥ | è°ƒæ•´ `gamma`, `subsample`, `colsample_bytree` | å¢å¼ºæ³›åŒ–èƒ½åŠ› |
| ç¬¬ä¸‰æ­¥ | é™ä½ `learning_rate`ï¼Œå¢å¤§ `n_estimators` | æå‡ç¨³å®šæ€§ |
| ç¬¬å››æ­¥ | ä½¿ç”¨äº¤å‰éªŒè¯è°ƒ `reg_alpha` å’Œ `reg_lambda` | æ§åˆ¶è¿‡æ‹Ÿåˆ |

> å°æ•°æ®é›†ï¼šmax_depth ä¸å®œå¤ªæ·±ï¼ˆ35ï¼‰ï¼Œlearning_rate é€‚ä¸­ï¼ˆ0.050.1ï¼‰
> 
> 
> å¤§æ•°æ®é›†ï¼šæ ‘å¯ä»¥æ·±ä¸€äº›ï¼Œn_estimators å¤šä¸€äº›ï¼Œä½†æ³¨æ„æ—¶é—´æˆæœ¬
> 

---

### å¦‚ä½•æŸ¥çœ‹å’Œå¯è§†åŒ–æ¯æ£µæ ‘ï¼Ÿ

XGBoost å†…ç½®äº†æ¨¡å‹æŸ¥çœ‹æ–¹æ³•ï¼š

```python
import joblib
import xgboost as xgb
import matplotlib.pyplot as plt

model = joblib.load("cost_model.pkl")
xgb.plot_tree(model, tree_idx=0, rankdir='LR')

plt.savefig("tree_0.png", dpi=500)
print("Tree saved as tree_0.png")

```

![tree_0.png](image/tree_0.png)

æ ¸å¿ƒçš„æ¨¡å‹è®­ç»ƒä»£ç train_model.py

```python

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import joblib
import numpy as np
import matplotlib.pyplot as plt

def train():
    """
    Loads the processed data, trains two separate XGBoost models for hours and cost,
    evaluates them, and saves the trained models to disk.
    """
    print("Starting model training process...")

    # 1. Load the processed data
    try:
        data_path = "./processed_modeling_data.csv"
        df = pd.read_csv(data_path)
        print(f"Successfully loaded processed data from {data_path}. Shape: {df.shape}")
    except FileNotFoundError:
        print(f"Error: Processed data file not found at {data_path}.")
        print("Please run the data_processor.py script first.")
        return

    # 2. Define features (X) and targets (y)
    # We will predict 'target_hours' and 'target_cost'
    # All other columns will be used as features, except for identifiers
    
    # Ensure target columns exist
    if 'target_hours' not in df.columns or 'target_cost' not in df.columns:
        print("Error: Target columns ('target_hours', 'target_cost') not found in the data.")
        return

    # Drop non-feature columns
    features = df.drop(columns=['target_hours', 'target_cost', 'quote_id', 'project_id', 'vendor_id', 'actual_id', 'status'])
    
    # Ensure all feature columns are numeric. This is a safeguard.
    # XGBoost requires numeric inputs.
    features = features.select_dtypes(include=np.number)
    
    target_hours = df['target_hours']
    target_cost = df['target_cost']

    print(f"Features for training: {features.columns.tolist()}")

    # --- Train Model for Target Hours ---
    print("\n--- Training model for Target Hours ---")
    
    # 3. Split data for the hours model
    X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(
        features, target_hours, test_size=0.2, random_state=42
    )

    # 4. Initialize and train the XGBoost Regressor for hours
    hours_model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1 # Use all available CPU cores
    )
    
    print("Training hours model...")
    hours_model.fit(X_train_h, y_train_h)

    # 5. Evaluate the hours model
    print("Evaluating hours model...")
    predictions_h = hours_model.predict(X_test_h)
    mae_h = mean_absolute_error(y_test_h, predictions_h)
    print(f"Mean Absolute Error (Hours Model): {mae_h:.2f} hours")

    # 6. Save the hours model
    joblib.dump(hours_model, 'hours_model.pkl')
    print("Hours model saved to hours_model.pkl")

    # --- Train Model for Target Cost ---
    print("\n--- Training model for Target Cost ---")

    # 3. Split data for the cost model
    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        features, target_cost, test_size=0.2, random_state=42
    )

    # 4. Initialize and train the XGBoost Regressor for cost
    cost_model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
    
    print("Training cost model...")
    cost_model.fit(X_train_c, y_train_c)

    # 5. Evaluate the cost model
    print("Evaluating cost model...")
    predictions_c = cost_model.predict(X_test_c)
    mae_c = mean_absolute_error(y_test_c, predictions_c)
    print(f"Mean Absolute Error (Cost Model): {mae_c:.2f} (in 10k units)")

    # 6. Save the cost model
    joblib.dump(cost_model, 'cost_model.pkl')
    print("Cost model saved to cost_model.pkl")
    
    print("\nModel training process complete!")

if __name__ == '__main__':
    train()

```

å…·ä½“ä»£ç è¯¦è§Githubï¼šhttps://github.com/RaveyShare/XGBoostLearnAndUser.git

å¾®ä¿¡å…¬ä¼—å·å¯ä»¥è·å–æ›´å¤šç²¾å½©å†…å®¹æ¬¢è¿æ‰«ç å…³æ³¨

![image.png](image/image.png)